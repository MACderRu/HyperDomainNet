{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509ba400-d154-4765-8630-f30a80bed123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/HyperDomainNet\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a0ecc8-26f0-4456-948e-f6f9a0c22192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from core.utils.class_registry import ClassRegistry\n",
    "from core.utils.common import load_clip, mixing_noise\n",
    "from core.utils.text_templates import imagenet_templates, imagenet_templates_small\n",
    "from core.utils.example_utils import Inferencer\n",
    "from core.utils.loss_utils import get_tril_elements_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ef9833-9130-4e29-a856-3e8a0acdd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, visual_encoder, device, bs=12, data_size=500):\n",
    "        self.device = device\n",
    "        self.batch_size = bs\n",
    "        self.data_size = data_size\n",
    "        self.clip_models = {\n",
    "            visual_encoder: load_clip(visual_encoder, device)\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_text(\n",
    "        self, clip_model: nn.Module, text: str, templates=imagenet_templates\n",
    "    ):\n",
    "        tokens = clip.tokenize(t.format(text) for t in templates).to(self.device)\n",
    "        text_features = clip_model.encode_text(tokens).detach()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode_image(self, clip_model: nn.Module, preprocess, imgs: torch.Tensor):\n",
    "        images = preprocess(imgs).to(self.device)\n",
    "        image_features = clip_model.encode_image(images).detach()\n",
    "        image_features /= image_features.clone().norm(dim=-1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    def _mean_cosine_sim(self, imgs_encoded: torch.Tensor, mean_vector: torch.Tensor):\n",
    "        return (imgs_encoded.unsqueeze(1) * mean_vector).sum(dim=-1).mean().item()\n",
    "\n",
    "    def _std_cosine_sim(self, imgs_encoded: torch.Tensor, mean_vector: torch.Tensor):\n",
    "        return nn.CosineSimilarity()(imgs_encoded, mean_vector).std().item()\n",
    "\n",
    "    def _diversity_from_embeddings_pairwise_cosines(self, imgs_encoded: torch.Tensor):\n",
    "        data = (imgs_encoded @ imgs_encoded.T).cpu().numpy()\n",
    "        mask = get_tril_elements_mask(data.shape[0])\n",
    "        return np.mean(1 - data[mask])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate_data(\n",
    "        self,\n",
    "        clip_model,\n",
    "        preprocess,\n",
    "        model\n",
    "    ):\n",
    "        answer = []\n",
    "\n",
    "        for idx in tqdm(range(self.data_size // self.batch_size)):\n",
    "            sample_z = mixing_noise(\n",
    "                self.batch_size,\n",
    "                512,\n",
    "                0,\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            _, trg_imgs = model(sample_z)\n",
    "            trg_imgs = trg_imgs.detach()\n",
    "            \n",
    "            image_features = self._encode_image(clip_model, preprocess, trg_imgs)\n",
    "            answer.append(image_features)\n",
    "\n",
    "        return torch.cat(answer, dim=0)\n",
    "\n",
    "    def get_metrics(\n",
    "        self, model, text_description\n",
    "    ):\n",
    "\n",
    "        model.eval()\n",
    "        metrics = {}\n",
    "        \n",
    "        for key, (clip_model, preprocess) in self.clip_models.items():\n",
    "            domain_mean_vector = self._encode_text(clip_model, text_description).unsqueeze(0)\n",
    "            imgs_encoded = self._generate_data(\n",
    "                clip_model,\n",
    "                preprocess,\n",
    "                model\n",
    "            )\n",
    "\n",
    "            key_quality = f\"quality/{text_description}/{key.replace('/', '-')}\"\n",
    "            key_diversity = f\"diversity/{text_description}/{key.replace('/', '-')}\"\n",
    "\n",
    "            metrics[key_quality] = self._mean_cosine_sim(imgs_encoded, domain_mean_vector)\n",
    "            metrics[key_diversity] = self._diversity_from_embeddings_pairwise_cosines(imgs_encoded)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c095d91-ddeb-4633-ad7b-3bad8418e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "ckpt_path = 'td_checkpoints/td_anime.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "model = Inferencer(ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b75ae7-bca5-4b69-8131-571310adb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator('ViT-B/16', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d375c7-263c-4f72-b39b-3954441c973a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9370a2a2f3eb45b5b6fbf79a7df4039d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = evaluator.get_metrics(model, 'Anime Painting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeceaeb6-d671-48dc-ad63-2aa62a56ddfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quality/Anime Painting/ViT-B-16': 0.2890625,\n",
       " 'diversity/Anime Painting/ViT-B-16': 0.2585}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31029fda-fd6c-40be-a799-8ea6a0842fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
